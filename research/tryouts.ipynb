{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\nishz\\\\Downloads\\\\medic.ai-main\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader, DirectoryLoader\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls = PyPDFLoader)\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(data='..//Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data\n",
    "\n",
    "# Chunking : \n",
    "def text_split(data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(data)\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks = text_split(extracted_data)\n",
    "print(f\"length of chunks : {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings \n",
    "\n",
    "def download_hf_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hf_embeddings()\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = embeddings.embed_query(\"hey there\")\n",
    "print(test)\n",
    "print(f\"Length : {len(test)}\") # 384 for 384 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone \n",
    "from pinecone import ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# Run only once to create index\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "pc = Pinecone(api_key = PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"tanishka\"\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension= 384, \n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name = \"tanishka\"\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name= index_name,\n",
    "    embedding= embeddings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "pc = Pinecone(api_key = PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"tanishka\"\n",
    "index = pc.Index(index_name)\n",
    "# docsearch = index.load(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above cell will store embeddings of every single chunk of our document.\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_name = \"tanishka\"\n",
    "# pc = Pinecone(api_key = PINECONE_API_KEY)\n",
    "# docsearch = pc.from_existing_index(\n",
    "#     name=index_name,\n",
    "#     dimension= 384, \n",
    "#     metric=\"cosine\",\n",
    "#     spec=ServerlessSpec(\n",
    "#         cloud=\"aws\",\n",
    "#         region=\"us-east-1\"\n",
    "#     )\n",
    "# )\n",
    "# docsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch.similarity_search(\"What is the side effect of paracetamol?\")\n",
    "docsearch.similarity_search(\"What is Acne, what causes acne ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever()\n",
    "# retriever.invoke(\"What is the side effect of paracetamol?\")\n",
    "retriever.invoke(\"What is Acne, what causes acne ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch.similarity_search(\"What is the side effect of paracetamol?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local LLMs For RAG Via LM Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI \n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gemma-2-2b-instruct\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a medical expert named Dr. Medic, You are not a doctor or a medical professor, but you know a lot about medicine, you can assume yourself as a medical wizard/expert,you are given a question and you need to answer it based on the provided context.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "# print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "def animated_print(text, delay=0.05):\n",
    "    for char in text:\n",
    "        sys.stdout.write(char)  \n",
    "        sys.stdout.flush()      \n",
    "        time.sleep(delay)       \n",
    "\n",
    "generated_text = completion.choices[0].message.content\n",
    "\n",
    "# animated_print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "# pinecone.init(\n",
    "#     api_key=\"your-pinecone-api-key\",\n",
    "#     environment=\"your-environment\"\n",
    "# )\n",
    "\n",
    "# Initialize local embeddings using a sentence transformer model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",  # This is a lightweight but effective model\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "# Initialize the vector store with local embeddings\n",
    "index_name = \"tanishka\"\n",
    "vectorstore = Pinecone.from_existing_index(index_name, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Initialize the LangChain wrapper for your local LLM\n",
    "llm = OpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model_name=\"gemma-2-2b-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"\n",
    "You are Dr. Kiri, A medical expert and an assistant for question-answering tasks.\n",
    "You are given a question and you need to answer it based on the retrieved context.\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "Answer sufficiently in about 100 words and be precise.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Define the RAG chain\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def enter_question():\n",
    "    try:\n",
    "        print(\"About to invoke the rag_chain\")\n",
    "        question = input(\"Enter your prompt: \")\n",
    "        for chunk in rag_chain.stream(question):\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "        print(\"\\nJust finished invoking the rag_chain\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Main loop\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        enter_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are Dr. Kiri, A medical expert and an assistant for question-answering tasks, \"\n",
    "    \"you are given a question and you need to answer it based on the retrieved context to answer.\"\n",
    "    \"If you don't know the answer, just say that you don't know. Don't try to make up an answer.\"\n",
    "    \"Answer concisely.\"\n",
    "    \"\\n\\n Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{question}\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
